{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "dga_predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oYLF9d5Q9YTW"
      },
      "outputs": [],
      "source": [
        "! pip install -r requirements.txt\n",
        "! pip install tensorflow-gpu\n",
        "! pip install tldextract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "colab_type": "code",
        "id": "R1GIM-HTQLd6",
        "outputId": "3a41a579-d0ee-4d6c-bdba-bb0fb038b8a8"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3D5rAyhKAk1S"
      },
      "outputs": [],
      "source": [
        "# Predicting Domain Generation Algorithms using LSTMs\n",
        "This notebook contains code for classifying domains as malicious or benign. The dataset consists of the Alexa Top 1M combined with 500,000 pre-generated malicious domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HTWRAmuv9XWx"
      },
      "outputs": [],
      "source": [
        "\"\"\"Generates data for train/test algorithms\"\"\"\n",
        "from datetime import datetime\n",
        "import StringIO\n",
        "from urllib import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import cPickle as pickle\n",
        "import os\n",
        "import random\n",
        "import tldextract\n",
        "\n",
        "# Our input file containg all the training data\n",
        "DATA_FILE = 'traindata.pkl'\n",
        "\n",
        "def get_data(force=False):\n",
        "    return pickle.load(open(DATA_FILE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VPY_lYljSrKI"
      },
      "outputs": [],
      "source": [
        "## What are Neural Networks?\n",
        "\n",
        "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. \n",
        "In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. \n",
        "\n",
        "Neural networks can adapt to changing input; so, the network generates the best possible result without needing to redesign the output criteria. \n",
        "\n",
        "The concept of neural networks, which has its roots in artificial intelligence, is swiftly gaining popularity.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1592/1*yGMk1GSKKbyKr_cMarlWnA.jpeg'>\n",
        "\n",
        "\n",
        "\n",
        "**Three fundamental components** of neural networks:\n",
        "\n",
        "1. **Structure** - what the neural network looks like, including all the mathematical functions involved, the number of inputs and outputs, and the parameters, called **weights** that the network has to learn.\n",
        "    \n",
        "2. **Loss Function** - a metric that tells us how good or bad the network's predictions are. \n",
        "3. **Optimizer** - the algorithm used for **learning the weights** that give the network the best predictions.\n",
        "\n",
        "\n",
        "### The Simplest Neural Network - The Perceptron\n",
        "The perceptron, arguably the simplest neural network, was invented by psychologist Frank Rosenblatt in 1957 and looks something like this:\n",
        "![perceptron](https://docs.google.com/uc?export=download&id=1SbHK9XPrP1PSO9T-lh9uG9CTCNjdXhU1)\n",
        "\n",
        "(image source: http://ataspinar.com/2016/12/22/the-perceptron/)\n",
        "\n",
        "A perceptron is basically a neural network with a single **artificial neuron**. Similar to the biological neuron, a perceptron has the following characteristics:\n",
        "\n",
        "- **inputs** - the perceptron receives a given number of real-valued inputs (the inputs are numbers).\n",
        "- **weights** - the perceptron has a weight $ w_i $ associated with each input $ x_i $. These weighted connections are like synapses and they are parameters that the perceptron must \"learn\".\n",
        "- **weighted sum (basically a dot product)** - the inputs are multiplied by the weights and the results are added together to produce a weighted sum.\n",
        "- **activation function** - the perceptron has an activation function called the unit-step function that produces an output of 1 if the weighted sum is greater than some threshold $\\theta$ and -1 otherwise.\n",
        "  \n",
        "\n",
        "*tanh*:\n",
        "tanh is like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
        "\n",
        "![tanh](https://miro.medium.com/max/744/1*f9erByySVjTjohfFdNkJYQ.jpeg)\n",
        "\n",
        "*Softmax*: \n",
        "Softmax function takes a vector as input and produces a vector of the same shape as the output. In a way, this function basically acts on an entire layer. The softmax function basically converts a vector of real values into a probability distribution and is useful for representing the probabilities of different classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Hidden layers** : layer of neurons other than the input and output layers\n",
        "\n",
        "**Dropout** : Technique to reduce overfitting in neural networks by shutting particular or random neurons at a point of time.\n",
        "\n",
        "**Loss function** :  Method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.\n",
        "\n",
        "*Cross-entropy loss*: measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "\n",
        "\n",
        "**Forward Pass**: The forward pass refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.\n",
        "\n",
        "**Backpropagation**:\n",
        "Backward pass refers to process of counting changes in weights, using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HbqlYKQbTG3C"
      },
      "outputs": [],
      "source": [
        "## What are Recurrent Neural Networks?\n",
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "\n",
        "RNNs are designed to take sequences of text as inputs or return sequences of text as outputs, or both. \n",
        "\n",
        "They're called recurrent because the network's hidden layers have a loop in which the output from one time step becomes an input at the next time step. This recurrence serves as a form of memory. \n",
        "\n",
        "It allows contextual information to flow through the network so that relevant outputs from previous time steps can be applied to network operations at the current time step. \n",
        "\n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-6eced51767f5bcd94b32bbe50da438e9\">\n",
        "\n",
        "# **Vanishing Gradient Problem **\n",
        "\n",
        "As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.\n",
        "\n",
        "A large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.\n",
        "\n",
        "A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network.\n",
        "\n",
        "\n",
        "\n",
        "## What are LSTMs (Long short-term memory)?\n",
        "\n",
        "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "\n",
        "A common LSTM unit is composed of a **cell**, an **input gate**, an **output gate** and a **forget gate**. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
        "\n",
        "RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png'>\n",
        "\n",
        "\n",
        "\n",
        "**The cell** : Responsible for keeping track of the dependencies between the elements in the input sequence. \n",
        "\n",
        "**The input gate** : Controls the extent to which a new value flows into the cell.\n",
        "\n",
        "**The forget gate**: Controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. \n",
        "\n",
        "The activation function of the LSTM gates is often the logistic sigmoid function.\n",
        "\n",
        "<img src='https://miro.medium.com/max/2840/1*0f8r3Vd-i4ueYND1CUrhMA.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "colab_type": "code",
        "id": "jQ-RoOsa9XXE",
        "outputId": "4299a608-005b-4929-c7a0-e35fda912c98"
      },
      "outputs": [],
      "source": [
        "\"\"\"Train and test LSTM classifier\"\"\"\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.optimizers import rmsprop\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_model(max_features, maxlen):\n",
        "    \"\"\"Build LSTM model\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop')\n",
        "\n",
        "    return model\n",
        "\n",
        "def run(max_epoch=25, nfolds=10, batch_size=128):\n",
        "    \"\"\"Run train/test on logistic regression model\"\"\"\n",
        "    indata = get_data()\n",
        "\n",
        "    # Extract data and labels\n",
        "    X = [x[1] for x in indata]\n",
        "    labels = [x[0] for x in indata]\n",
        "\n",
        "    # Generate a dictionary of valid characters\n",
        "    valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}\n",
        "\n",
        "    max_features = len(valid_chars) + 1\n",
        "    maxlen = np.max([len(x) for x in X])\n",
        "\n",
        "    # Convert characters to int and pad\n",
        "    X = [[valid_chars[y] for y in x] for x in X]\n",
        "    X = sequence.pad_sequences(X, maxlen=maxlen)\n",
        "\n",
        "    # Convert labels to 0-1\n",
        "    y = [0 if x == 'benign' else 1 for x in labels]\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    for fold in range(nfolds):\n",
        "        print \"fold %u/%u\" % (fold+1, nfolds)\n",
        "        X_train, X_test, y_train, y_test, _, label_test = train_test_split(X, y, labels, \n",
        "                                                                           test_size=0.2)\n",
        "\n",
        "        print 'Build model...'\n",
        "        #model = build_model(max_features, maxlen)\n",
        "        model = load_model('lstm.h5')\n",
        "\n",
        "        print \"Train...\"\n",
        "        X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)\n",
        "        best_iter = -1\n",
        "        best_auc = 0.0\n",
        "        out_data = {}\n",
        "\n",
        "        for ep in range(max_epoch):\n",
        "            model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1)\n",
        "\n",
        "            t_probs = model.predict_proba(X_holdout)\n",
        "            t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)\n",
        "\n",
        "            print 'Epoch %d: auc = %f (best=%f)' % (ep, t_auc, best_auc)\n",
        "\n",
        "            if t_auc > best_auc:\n",
        "                best_auc = t_auc\n",
        "                best_iter = ep\n",
        "\n",
        "                probs = model.predict_proba(X_test)\n",
        "\n",
        "                out_data = {'y':y_test, 'labels': label_test, 'probs':probs, 'epochs': ep,\n",
        "                            'confusion_matrix': sklearn.metrics.confusion_matrix(y_test, probs > .5)}\n",
        "\n",
        "                print sklearn.metrics.confusion_matrix(y_test, probs > .5)\n",
        "            else:\n",
        "                # No longer improving...break and calc statistics\n",
        "                if (ep-best_iter) > 2:\n",
        "                    break\n",
        "\n",
        "        final_data.append(out_data)\n",
        "        model.save('lstm.h5')\n",
        "\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mo_JbzKPKTpn"
      },
      "outputs": [],
      "source": [
        "See the results from the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "colab_type": "code",
        "id": "kovhot9N9XXN",
        "outputId": "dd1ede2f-2586-468e-9e75-ddbc637ea45c"
      },
      "outputs": [],
      "source": [
        "\"\"\"Run experiments and create figs\"\"\"\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import numpy as np\n",
        "\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "RESULT_FILE = 'results.pkl'\n",
        "\n",
        "def run_experiments(nfolds=10):\n",
        "    \"\"\"Runs all experiments\"\"\"\n",
        "    lstm_results = None\n",
        "\n",
        "    lstm_results = run(nfolds=nfolds)\n",
        "\n",
        "    return lstm_results\n",
        "\n",
        "def create_figs(nfolds=10, force=False):\n",
        "    \"\"\"Create figures\"\"\"\n",
        "    # Generate results if needed\n",
        "    if force or (not os.path.isfile(RESULT_FILE)):\n",
        "        lstm_results = run_experiments(nfolds)\n",
        "\n",
        "        results = {'lstm': lstm_results}\n",
        "\n",
        "        pickle.dump(results, open(RESULT_FILE, 'w'))\n",
        "    else:\n",
        "        results = pickle.load(open(RESULT_FILE))\n",
        "\n",
        "    # Extract and calculate LSTM ROC\n",
        "    if results['lstm']:\n",
        "        lstm_results = results['lstm']\n",
        "        fpr = []\n",
        "        tpr = []\n",
        "        for lstm_result in lstm_results:\n",
        "            t_fpr, t_tpr, _ = roc_curve(lstm_result['y'], lstm_result['probs'])\n",
        "            fpr.append(t_fpr)\n",
        "            tpr.append(t_tpr)\n",
        "        lstm_binary_fpr, lstm_binary_tpr, lstm_binary_auc = calc_macro_roc(fpr, tpr)\n",
        "\n",
        "    # Save figure\n",
        "    from matplotlib import pyplot as plt\n",
        "    with plt.style.context('bmh'):\n",
        "        plt.plot(lstm_binary_fpr, lstm_binary_tpr,\n",
        "                 label='LSTM (AUC = %.4f)' % (lstm_binary_auc, ), rasterized=True)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate', fontsize=22)\n",
        "        plt.ylabel('True Positive Rate', fontsize=22)\n",
        "        plt.title('ROC - Binary Classification', fontsize=26)\n",
        "        plt.legend(loc=\"lower right\", fontsize=22)\n",
        "\n",
        "        plt.tick_params(axis='both', labelsize=22)\n",
        "        plt.savefig('results.png')\n",
        "\n",
        "def calc_macro_roc(fpr, tpr):\n",
        "    \"\"\"Calcs macro ROC on log scale\"\"\"\n",
        "    # Create log scale domain\n",
        "    all_fpr = sorted(itertools.chain(*fpr))\n",
        "\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(len(tpr)):\n",
        "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "    return all_fpr, mean_tpr / len(tpr), auc(all_fpr, mean_tpr) / len(tpr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_figs(nfolds=1) # Run with 1 to make it fast"
      ]
    }
  ]
}