{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "AIS_CSG_COLAB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "807XTIbMe6TG",
        "colab_type": "text"
      },
      "source": [
        "# **Using a GAN to create malicious domains**\n",
        "Just as machine-learning algorithms can be used to identify malicious domains, they can also be used to create domains that incorrectly-pass those checks. A popular method to generate such domains is using a Generative-Adversarial Network, otherwise known as a GAN.\n",
        "\n",
        "## **How does a GAN work?**\n",
        "A GAN is comprised of two models - a generator and a discriminator. These two models are pitted against each other. The generator tries to fool the discriminator by generating false input, and the discriminator tries to correctly identify the generator's false input mixed among correct ones.\n",
        "\n",
        "![Image showing GAN network diagram](https://skymind.ai/images/wiki/gan_schema.png)\n",
        "Credit: https://skymind.ai/images/wiki/gan_schema.png\n",
        "\n",
        "A noise vector (vector of random numbers) is passed into the generator, which uses the noise to generate an output. That output is combined with actual output from a dataset and shuffled then passed to the discriminator. The discriminator predicts where the images came from - the generator or the true dataset.\n",
        "\n",
        "## **Examples of GANs being used**\n",
        "GANs have been used to create photorealistic images, cartoons, and more. Some examples:\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-Translation-from-Paintings-to-Photographs-with-CycleGAN.png)\n",
        "https://junyanz.github.io/CycleGAN/\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-Textual-Descriptions-and-GAN-Generated-Photographs-if-Birds-and-Flowers.png)\n",
        "[Generative Adversarial Text to Image Synthesis](https://arxiv.org/abs/1605.05396)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPCfIxJ_0_KM",
        "colab_type": "text"
      },
      "source": [
        "## Using GANs on Domains\n",
        "We use Amazon's \"Top 1 Million Sites\" dataset for the domains of 1 million legitimate sites. This serves as the legitimate domains that the discriminator would have to distinguish from the fake domains generated by the generator.\n",
        "\n",
        "First, we import the necessary libraries. Pandas and scikit-learn (sklearn) are used to perform preprocessing on the top 1-million-sites CSV. Keras is used to create and train the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMw3or53Dq7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwR7WxroDq7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model, load_model, clone_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Embedding, Dense, Conv1D, Input, GlobalMaxPooling1D, LSTM, Concatenate, TimeDistributed, MaxPooling1D, BatchNormalization, Reshape, Permute, Lambda\n",
        "from keras.backend import int_shape, concatenate, transpose, permute_dimensions, expand_dims\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import RepeatVector, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haBLDEnmDq75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = pd.read_csv('top-1m.csv')\n",
        "csv_file.columns = [\"id\",\"domain\"] #this line is somehow removing \"google.com\" which is the first line so add it back\n",
        "domains = [d.split('.')[0] for d in csv_file['domain']][:50000] #remove tld"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "851LbxFqDq8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = Counter(\"\".join(domains))\n",
        "keys, values = zip(*sorted(c.items()))\n",
        "\n",
        "indexes = np.arange(len(keys))\n",
        "width = 1\n",
        "\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "plt.bar(indexes, values, width)\n",
        "plt.xticks(indexes + width * 0.5, keys)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWFxnrlg5MLx",
        "colab_type": "text"
      },
      "source": [
        "## Machine learning algorithms cannot process text directly\n",
        "Using the ASCII codes of characters does not properly represent the distinction between characters.\n",
        "\n",
        "\n",
        "```\n",
        "1 = 'a', 25 = 'y'. 1 + 2 = 3? 'ay'?\n",
        "```\n",
        "A better example might be this: let's represent an apple as 1, and a banana as 2. If we perform any mathematical operations, what is the result? Perhaps 1 + 2 = smoothie?\n",
        "\n",
        "Since machine-learning algorithms typically depend on having numbers as input to perform their mathematical operations, this poses an issue. One of the ways to circumvent this is to convert the categories to different columns of boolean value.\n",
        "\n",
        "For example:\n",
        "\n",
        "\n",
        "```\n",
        "Table:\n",
        "| 'Letter' |        |    'a'   |    'b'   |    'c'   |\n",
        "------------        |--------------------------------|\n",
        "|    'a'   | =====> |     1    |     0    |     0    |\n",
        "|    'b'   |        |     0    |     1    |     0    |\n",
        "|    'c'   |        |     0    |     0    |     1    |\n",
        "\n",
        "```\n",
        "\n",
        "This converts the categorical column to several categories indicating whether that certain value exists. This better-represents the column when used for mathematical operations. This procedure is typically known as one-hot-encoding.\n",
        "\n",
        "To perform this for the domains, we use Keras's preprocessing module Tokenizer. It converts the alphabet provided into tokens, which is put through the tokenizer to convert the domain into a 2D array where the rows are the letter indices for the domain and the columns are which token is the letter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHfI3SbUDq8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tk = Tokenizer(num_words=None, char_level=True, oov_token=None)\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-_ \"#,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i\n",
        "tk.word_index = char_dict.copy()\n",
        "vocab_size=len(tk.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akoukzqKDq8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tk.texts_to_sequences(domains[0]) # --> prints out sequence for \"youtube\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvSzfeytDq8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_categorical(domain_sequence):\n",
        "    m,n = domain_sequence.shape\n",
        "    out = np.zeros((m,n,vocab_size))\n",
        "    for i, seq in enumerate(domain_sequence):\n",
        "        #one_sample = np.zeros((len(seq), len(alphabet)))\n",
        "        for j, value in enumerate(seq):\n",
        "            out[i][j][value] = 1\n",
        "        #out.append(one_sample)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKi2XeHB5wNy",
        "colab_type": "text"
      },
      "source": [
        "Tokenization requires a set-length input, since it needs to know how many letters exist to create the 2D array. We concatenate all the 2D arrays generated by putting domains into the tokenizer to get a 3D array, where each layer is a domain tokenized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfzVtncWDq8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 15#max([len(i) for i in domains]) #we have to cap the  length of domain to an arbitrary number, pad all the domains having lesser than 15 chars\n",
        "domain_sequence = tk.texts_to_sequences(domains)\n",
        "padded_sequence = sequence.pad_sequences(domain_sequence, maxlen=max_len, value=vocab_size-1)\n",
        "train_data = get_categorical(padded_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO-FgIm86D5o",
        "colab_type": "text"
      },
      "source": [
        "The input domains are separated into training (for training the model) and testing (for calculating how good our model is). We used 80% of the dataset set for training and 20% set for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfKUAiIjDq8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cut_off = int(len(train_data)*0.8)\n",
        "np.random.shuffle(train_data)\n",
        "X_train, X_test = train_data[:cut_off] , train_data[cut_off:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ICcuio61wN",
        "colab_type": "text"
      },
      "source": [
        "## Creating the GAN\n",
        "If we just put in the tokenized domain into a GAN and expected it to easily generate fake tokenized domains with a good discriminator, we would find that the training would be highly-inefficient. This is because domains are maximum length 65. With 39 possible characters in each index in a domain, that would result in **65 * 39 = 2535 inputs per domain!** Also, domains are different lengths. This is hard for a GAN that needs a set number of inputs to deal with.\n",
        "\n",
        "To make this smaller, we use an _autoencoder_. It turns out that it is often possible to compress an array of numbers into a smaller-size array while still maintaining most of the important features of the array. This array of features is less-susceptible to being affected by different input lengths. Autoencoders perform this by having a \"bottleneck\" in the model.\n",
        "\n",
        "![Autoencoder diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png)\n",
        "\n",
        "An autoencoder consists of two parts - an encoder and a decoder. The encoder converts the input (X) to a smaller array of features (z). The decoder performs the reverse - it takes the smaller array and tries to recreate the input (X').\n",
        "\n",
        "Our autoencoder consists of an LSTM (encoder) to RepeatVector (repeats the input to LSTM to gather full sequence information) to LSTM and a Dense (decoder). The encoder returns a LSTM output of size 256, which is the compressed version of our input domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG7fxfKNDq8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_input = Input(shape=(None, vocab_size), name='main_input')\n",
        "lstm = LSTM(units=256, return_sequences=False)(main_input)\n",
        "encoder = Model(inputs=[main_input], outputs=[lstm])\n",
        "\n",
        "r = RepeatVector(max_len)(lstm)\n",
        "lstm2 = LSTM(units=256, return_sequences=True)(r)\n",
        "decoder_output =  Dense(vocab_size, activation='softmax')(lstm2)\n",
        "autoencoder = Model(inputs=[main_input], outputs=[decoder_output])\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE4XGGI9AspQ",
        "colab_type": "text"
      },
      "source": [
        "## Training an autoencoder\n",
        "An autoencoder is trained by passing in an input and expecting the same as output (for a successful encoder and decoder). Here, we pass in our training domains and look for similar domains as output. We also save the model so that we do not need to repeat training every time our code is run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Pa04xGJUDq8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "autoencoder.fit(X_train, X_train, epochs=20, batch_size=256, shuffle=True, validation_data=(X_test, X_test))\n",
        "autoencoder.save('ais-csg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_daV_jzzDq83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder = load_model('ais-csg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsdKp3XgBStQ",
        "colab_type": "text"
      },
      "source": [
        "Comparing the output of the autoencoder with its input, we see that they are extremely similar. Looks like the autoencoder successfully converts the 15 * 39 size array to a 256-size array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENtPDq3zDq87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = X_test[20:30]\n",
        "predictions = autoencoder.predict(test_data)\n",
        "for i, pred in enumerate(predictions):\n",
        "    domain, pred_domain = \"\", \"\"\n",
        "    for j,row in enumerate(pred):\n",
        "        char, pred_char = alphabet[np.argmax(test_data[i][j])], alphabet[np.argmax(row)]\n",
        "        if char != \" \":\n",
        "            domain+=char\n",
        "        if pred_char != \" \":\n",
        "            pred_domain+=pred_char\n",
        "    print(domain, \" \", pred_domain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8wXn2EgCK4I",
        "colab_type": "text"
      },
      "source": [
        "With a trained autoencoder, we freeze it so that it can be inserted into the GAN without being trained along with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZCPCuzLDq8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in autoencoder.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt83ciOmDGMu",
        "colab_type": "text"
      },
      "source": [
        "## Creating the GAN Generator and Discriminator\n",
        "As stated before, a GAN is comprised otwo parts - a generator and a discriminator. We have our generator use the decoder to take the \"compressed\" output of the generator's output to actual domain name. The generator converts the input to what is best to fool the discriminator by using a Dense layer before passing it to the decoder.\n",
        "\n",
        "The discriminator does the opposite - it uses the encoder to create an easier-to-understand input from the domain which it then evaluates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnbahxjrDq9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator is made from decoder\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, trainable=True, activation='relu'))\n",
        "generator.add(autoencoder.layers[-3])\n",
        "generator.add(autoencoder.layers[-2])\n",
        "generator.add(autoencoder.layers[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QHAwE1wDq9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Sequential()\n",
        "discriminator.add(autoencoder.layers[1])\n",
        "discriminator.add(Dense(256, trainable=True, activation='relu'))\n",
        "discriminator.add(Dense(1, trainable=True, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9hWGARkDq9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in generator.layers:\n",
        "    print(layer.trainable)\n",
        "\n",
        "for layer in discriminator.layers:\n",
        "    print(layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzh4bbacD4oV",
        "colab_type": "text"
      },
      "source": [
        "Combining the discriminator and the generator creates the GAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FedTm4wdDq9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_input = Input(shape=(20, ))\n",
        "x = generator(gan_input)\n",
        "gan_output = discriminator(x)\n",
        "gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='Adam')\n",
        "gan.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPoYrvckEEjN",
        "colab_type": "text"
      },
      "source": [
        "Some helper functions we made. generate\\_samples() returns random domains. show_samples() prints out samples generated by generator while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1a_3M4uDq9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_samples(input=None):\n",
        "    noise_samples = np.random.rand(10, 20)\n",
        "    generated_domains = generator.predict_on_batch(noise_samples)\n",
        "    domains = []\n",
        "    for i, pred in enumerate(generated_domains):\n",
        "        domain = \"\"\n",
        "        for j,row in enumerate(pred):\n",
        "            char = alphabet[np.argmax(row)]\n",
        "            if char != \" \":\n",
        "                domain+=char\n",
        "        domains.append(domain)\n",
        "    return domains"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smuZ0bivtf1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_samples():\n",
        "  noise_samples = np.random.rand(10, 20)\n",
        "  generated_domains = generator.predict_on_batch(noise_samples)\n",
        "  for i, pred in enumerate(generated_domains):\n",
        "    domain = \"\"\n",
        "    for j,row in enumerate(pred):\n",
        "      char = alphabet[np.argmax(row)]\n",
        "      if char != \"\":\n",
        "        domain+=char\n",
        "    print(domain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kZpIzVadZb0",
        "colab_type": "text"
      },
      "source": [
        "## The issue with GANs\n",
        "GANs are incredibly-powerful but come with a large issue - mode collapse. This occurs when either the generator or the discriminator learns better than the other, which results in the other model constantly being outdone and tries random actions to overcome the gap (which do not work).\n",
        "\n",
        "When we first tried to train the GAN, we discovered that the generator always outperformed the discriminator by far. So we tried to improve this by giving the discriminator a head-start with extra training at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdewR5LsZS3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(500):\n",
        "  output = generator.predict(np.random.rand(100, 20))\n",
        "  true = X_train[random.sample(range(0, len(X_train)), 100)]\n",
        "  X = np.vstack((output, true))\n",
        "  y = np.vstack((np.ones((100,1)), np.zeros((100,1))))\n",
        "  loss = discriminator.train_on_batch(X,y)\n",
        "  print(\"current loss for discriminator is \",loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hzjx8E6Dq9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator.trainable = False\n",
        "for i in range(3):\n",
        "    X = np.random.rand(100,20)\n",
        "    y = np.zeros(100)\n",
        "    loss = gan.train_on_batch(X,y)\n",
        "    print(\"current loss for gan is \", loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB5y-rW0eJ19",
        "colab_type": "text"
      },
      "source": [
        "Another method we used to reduce chances of mode collapse is to have a copy of the generator on the previous iteration put its results alongside the currently-best generator (which has had a few more more training cycles)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMEgxe1XDq9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_copy = clone_model(generator)\n",
        "generator_copy.set_weights(generator.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVaHeqLeY-5",
        "colab_type": "text"
      },
      "source": [
        "## Training the GAN\n",
        "Training a GAN involves several steps. For every iteration of training, the generator and the discriminator need to be trained - _but not at the same time_. This is because their loss functions depend on each other - a discriminator cannot judge how much better it has done than before if the generator is being changed during the run (and vice versa for the generator).\n",
        "\n",
        "Instead, we train the discriminator with generated domains from the generator and real domains randomly-sampled from the dataset. Then, we freeze the discriminator and train the generator based on how good was the discriminator output. Repeating this, the GAN can be trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK9dneFADq9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 200\n",
        "num_epochs = 100\n",
        "num_batches = int(len(X_train)/batch_size)\n",
        "count=1\n",
        "for i in range(num_epochs):\n",
        "    #for j in range(0, num_batches, batch_size):    \n",
        "    for j in range(2):\n",
        "        discriminator.trainable = True\n",
        "        noise_recent_samples = generator.predict(np.random.rand(100, 20))\n",
        "        noise_past_samples = generator_copy.predict(np.random.rand(100,20))\n",
        "        real_samples = X_train[random.sample(range(0, len(X_train)), 200)]\n",
        "        X = np.vstack((noise_recent_samples, noise_past_samples, real_samples))\n",
        "        y = np.vstack((np.ones((200,1)), np.zeros((200,1))))\n",
        "        X,y = shuffle(X,y)\n",
        "        loss = discriminator.train_on_batch(X,y)\n",
        "        print(\"current loss for discriminator is \",loss)\n",
        "        \n",
        "    if (count%10==0):\n",
        "        generator_copy.set_weights(generator.get_weights())\n",
        "        show_samples()\n",
        "    \n",
        "    for t in range(1):\n",
        "        discriminator.trainable = False\n",
        "        X = np.random.rand(100, 20)\n",
        "        y = np.zeros(int(batch_size/2))\n",
        "        loss = gan.train_on_batch(X,y)\n",
        "        print(\"current loss for gan is \", loss)\n",
        "    \n",
        "    count+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n85LBZn7fTOR",
        "colab_type": "text"
      },
      "source": [
        "## Those generated domains don't seem to be very realistic . . .\n",
        "You can see that generator's and discriminator's losses flip-flopped throughout the training - the generator would collapse then the discriminator would collapse. This indicates that the two models are relatively even, which is what we want.\n",
        "\n",
        "Yet, we have obviously-fake output. This is where we run into the second issue for GANs - they are incredibly unstable. Many times, the same training would result in wildly-different outputs just because the random seed was different. This is the main issue holding GANs back. There have been papers applying GANs to domain-generating algorithms, which indicates that GANs can be used here. Yet, we could not replicate those results mostly due to the instability of GANs. In other words, GANs are extremely powerful yet extremely volatile at the same time.\n",
        "\n",
        "There has been many research into techniques to make GANs more stable. For example, we could have used a pre-trained discriminator to train a generator against which would find a stable solution."
      ]
    }
  ]
}